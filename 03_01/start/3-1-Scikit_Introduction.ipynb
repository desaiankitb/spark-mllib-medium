{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Introduction\n",
    "\n",
    "Scikit-learn is a library for machine learning in Python.  It helps us in all of the following ways:\n",
    "\n",
    "    1. Preprocessing: Getting the data into shape for Machine Learning\n",
    "    2. Dimensionality Reduction: Reducing redundancy in variables\n",
    "    3. Classification: Predicting one of a finite set of classes for data.\n",
    "    4. Regression: Predicting a response variable\n",
    "    5. Clustering: Finding natural patterns in the data.\n",
    "    6. Model Selection: Finding the best model for our data.\n",
    "\n",
    "\n",
    "We will be looking at our NYCFlights13 dataset here.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cross_validation  import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "flights = pd.read_csv('../../data/nycflights13/flights.csv.gz')\n",
    "weather = pd.read_csv('../../data/nycflights13/weather.csv.gz')\n",
    "airports = pd.read_csv('../../data/nycflights13/airports.csv.gz')\n",
    "\n",
    "df_withweather = pd.merge(flights, weather, how='left', on=['year','month', 'day', 'hour'])\n",
    "df = pd.merge(df_withweather, airports, how='left', left_on='dest', right_on='faa')\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's examine the data\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "\n",
    "Let's create a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pred = 'dep_delay'\n",
    "features =  ['month','day','dep_time','arr_time','carrier','dest','air_time','distance', \n",
    "             'lat', 'lon', 'alt',  'dewp', 'humid', 'wind_speed', 'wind_gust', \n",
    "             'precip', 'pressure', 'visib' ]\n",
    "\n",
    "features_v = df[features]\n",
    "pred_v = df[pred]\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# carrier is not a number, so transform it into an number\n",
    "features_v['carrier'] = pd.factorize(features_v['carrier'])[0]\n",
    "\n",
    "# dest is not a number, so transform it into a number\n",
    "features_v['dest'] = pd.factorize(features_v['dest'])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at our feature vector\n",
    "\n",
    "features_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the feature vector\n",
    "\n",
    "Let's scale the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notice how the magnitude of the dimensions is wildly different. Let's try scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Dimensions\n",
    "\n",
    "Let's use PCA to reduce dimensions down to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(scaled_features).transform(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "Let's do a quick plot of the data. Because we have many dimensions and we want a 2-D plot, we need to reduce dimensions down to 2.  We can do this with PCA, which will reduce the dimensions to only two by combining redundant features into two principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "plt.scatter(X_r[:,0], X_r[:,1], alpha=.8, lw=lw)\n",
    "plt.title('PCA of flights dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
